{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75d71ea5-e316-4f76-9f8e-da336bc1fb50",
   "metadata": {},
   "source": [
    "## Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61d08ca6-bdf2-4399-9e35-99aa2083457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf2fd86c-9c5f-4403-a2ae-37ac94df9662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Hello, welcome to my world!'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "translator = pipeline('translation_zh_to_en', model=model, tokenizer=tokenizer)\n",
    "\n",
    "translator(\"你好呀，欢迎来到我的世界！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26da400-a443-494e-b37c-803cfc538ca6",
   "metadata": {},
   "source": [
    "## Text 2 Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9fe23722-4a31-4f71-95b4-8b56cb5bd817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, os, sys, glob\n",
    "import torch\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, trange\n",
    "from einops import rearrange\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.models.diffusion.plms import PLMSSampler\n",
    "from munch import Munch\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a1b18d5-1bce-45fc-acde-a9585f25c92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_config(config, ckpt, verbose=False):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    if len(m) > 0 and verbose:\n",
    "        print(\"missing keys:\")\n",
    "        print(m)\n",
    "    if len(u) > 0 and verbose:\n",
    "        print(\"unexpected keys:\")\n",
    "        print(u)\n",
    "\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1afc2d0f-e339-4857-a93b-f98b97a6cfd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': {'base_learning_rate': 5e-05, 'target': 'ldm.models.diffusion.ddpm.LatentDiffusion', 'params': {'linear_start': 0.00085, 'linear_end': 0.012, 'num_timesteps_cond': 1, 'log_every_t': 200, 'timesteps': 1000, 'first_stage_key': 'image', 'cond_stage_key': 'caption', 'image_size': 32, 'channels': 4, 'cond_stage_trainable': True, 'conditioning_key': 'crossattn', 'monitor': 'val/loss_simple_ema', 'scale_factor': 0.18215, 'use_ema': False, 'unet_config': {'target': 'ldm.modules.diffusionmodules.openaimodel.UNetModel', 'params': {'image_size': 32, 'in_channels': 4, 'out_channels': 4, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_heads': 8, 'use_spatial_transformer': True, 'transformer_depth': 1, 'context_dim': 1280, 'use_checkpoint': True, 'legacy': False}}, 'first_stage_config': {'target': 'ldm.models.autoencoder.AutoencoderKL', 'params': {'embed_dim': 4, 'monitor': 'val/rec_loss', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}}, 'cond_stage_config': {'target': 'ldm.modules.encoders.modules.BERTEmbedder', 'params': {'n_embed': 1280, 'n_layer': 32}}}}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = OmegaConf.load(\"configs/latent-diffusion/txt2img-1p4B-eval.yaml\")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94e985f6-d7fe-4643-9146-a1ac579bcacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from models/ldm/text2img-large/model.ckpt\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 872.30 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    }
   ],
   "source": [
    "model = load_model_from_config(config, \"models/ldm/text2img-large/model.ckpt\")\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c58b886f-3ab4-4ba9-8644-d92030b1bf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_img(prompt):\n",
    "    opt = Munch({\n",
    "        \"plms\": False,\n",
    "        \"outdir\": './outputs/txt2img-samples',\n",
    "        \"ddim_steps\": 200,\n",
    "        \"ddim_eta\": 0.0,\n",
    "        \"n_iter\": 1,\n",
    "        \"H\": 256,\n",
    "        \"W\": 256,\n",
    "        \"n_samples\": 4,\n",
    "        \"scale\": 5.0\n",
    "    })\n",
    "\n",
    "    if opt.plms:\n",
    "        sampler = PLMSSampler(model)\n",
    "    else:\n",
    "        sampler = DDIMSampler(model)\n",
    "\n",
    "    os.makedirs(opt.outdir, exist_ok=True)\n",
    "    outpath = opt.outdir\n",
    "\n",
    "\n",
    "\n",
    "    sample_path = os.path.join(outpath, \"samples\")\n",
    "    os.makedirs(sample_path, exist_ok=True)\n",
    "    base_count = len(os.listdir(sample_path))\n",
    "\n",
    "    all_samples=list()\n",
    "    with torch.no_grad():\n",
    "        with model.ema_scope():\n",
    "            uc = None\n",
    "            if opt.scale != 1.0:\n",
    "                uc = model.get_learned_conditioning(opt.n_samples * [\"\"])\n",
    "            for n in trange(opt.n_iter, desc=\"Sampling\"):\n",
    "                c = model.get_learned_conditioning(opt.n_samples * [prompt])\n",
    "                shape = [4, opt.H//8, opt.W//8]\n",
    "                samples_ddim, _ = sampler.sample(S=opt.ddim_steps,\n",
    "                                                 conditioning=c,\n",
    "                                                 batch_size=opt.n_samples,\n",
    "                                                 shape=shape,\n",
    "                                                 verbose=False,\n",
    "                                                 unconditional_guidance_scale=opt.scale,\n",
    "                                                 unconditional_conditioning=uc,\n",
    "                                                 eta=opt.ddim_eta)\n",
    "\n",
    "                x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
    "                x_samples_ddim = torch.clamp((x_samples_ddim+1.0)/2.0, min=0.0, max=1.0)\n",
    "\n",
    "                for x_sample in x_samples_ddim:\n",
    "                    x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                    # Image.fromarray(x_sample.astype(np.uint8)).save(os.path.join(sample_path, f\"{base_count:04}.png\"))\n",
    "                    base_count += 1\n",
    "                all_samples.append(x_samples_ddim)\n",
    "\n",
    "\n",
    "    # additionally, save as grid\n",
    "    grid = torch.stack(all_samples, 0)\n",
    "    grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "    grid = make_grid(grid, nrow=opt.n_samples)\n",
    "\n",
    "    # to image\n",
    "    grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "    produce_img = Image.fromarray(grid.astype(np.uint8))\n",
    "    return produce_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "465cf8bd-497a-4082-933f-8a43a9ca8999",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "32afe06d-f615-4c15-9cfc-0ba3ef09d742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861/\n",
      "Running on public URL: https://18083.gradio.app\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting, check out Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://18083.gradio.app\" width=\"900\" height=\"500\" allow=\"autoplay; camera; microphone;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<gradio.routes.App at 0x7f9a934134c0>,\n",
       " 'http://127.0.0.1:7861/',\n",
       " 'https://18083.gradio.app')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/ldm/lib/python3.8/site-packages/gradio/routes.py\", line 255, in run_predict\n",
      "    output = await app.blocks.process_api(\n",
      "  File \"/opt/conda/envs/ldm/lib/python3.8/site-packages/gradio/blocks.py\", line 599, in process_api\n",
      "    predictions, duration = await self.call_function(fn_index, processed_input)\n",
      "  File \"/opt/conda/envs/ldm/lib/python3.8/site-packages/gradio/blocks.py\", line 514, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/opt/conda/envs/ldm/lib/python3.8/site-packages/anyio/to_thread.py\", line 31, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/opt/conda/envs/ldm/lib/python3.8/site-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/opt/conda/envs/ldm/lib/python3.8/site-packages/anyio/_backends/_asyncio.py\", line 867, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/opt/conda/envs/ldm/lib/python3.8/site-packages/gradio/interface.py\", line 486, in <lambda>\n",
      "    lambda *args: self.run_prediction(args)[0]\n",
      "  File \"/opt/conda/envs/ldm/lib/python3.8/site-packages/gradio/interface.py\", line 666, in run_prediction\n",
      "    prediction = self.fn(*processed_input)\n",
      "  File \"/tmp/ipykernel_3153/3902717883.py\", line 7, in greet\n",
      "    prompt = translator(zh_inputs)[0]['translation_text']\n",
      "NameError: name 'zh_inputs' is not defined\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def greet(zh_input, en_input):\n",
    "    if en_input:\n",
    "        prompt = en_input\n",
    "    else:\n",
    "        prompt = translator(zh_input)[0]['translation_text']\n",
    "        \n",
    "    \n",
    "    return generate_img(prompt)\n",
    "\n",
    "demo = gr.Interface(fn=greet,\n",
    "                    title=\"Text2Image\",\n",
    "                    description=\"中文和英文只要输入任意一个即可，如果同时输入，以英文为主！\",\n",
    "                    inputs=[\n",
    "                        gr.Text(placeholder=\"请输入中文信息\", label='zh'),\n",
    "                        gr.Text(placeholder=\"请输入英文信息\", label='en')\n",
    "                           ],\n",
    "                    outputs=gr.Image(shape=(200, 200)),\n",
    "                    examples=[\n",
    "                        [\"穿皮鞋的熊猫在打篮球\", \"\"],\n",
    "                        [\"\", \"Green ducks dance on the red roof\"]\n",
    "                    ])\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2c2803-4909-44d4-9177-04bbaa9f81c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "ldm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
